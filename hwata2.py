# -*- coding: utf-8 -*-
"""hwata2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uDwzd-oC9P7lZwm_r0OShX6ER4VShkpo
"""

# hwata2.py
# HWATA 2: Streamlit App for Regularized Building Footprint Extraction (improved model)

import os
import hashlib
from tempfile import NamedTemporaryFile

import requests
import streamlit as st
import geopandas as gpd
import rasterio
import numpy as np
import torch
from rasterio import features
import segmentation_models_pytorch as smp
from buildingregulariser import regularize_geodataframe
import folium
from streamlit_folium import st_folium


# ====================== CONFIG ======================
# Replace with your actual Release asset URL
MODEL_URL = (
    "https://github.com/ck1972/hwata1-app/releases/download/v0.2.0/"
    "combined_hwata2_unet_transfer.pth"
)
MODEL_PATH = "combined_hwata2_unet_transfer.pth"
MODEL_SHA256 = ""  # optional: paste SHA-256 for integrity check


# ====================== PAGE ======================
st.set_page_config(page_title="HWATA 2 ‚Äì Improved Building Footprint Extractor", layout="centered")
st.title("üè† HWATA 2 ‚Äì Improved Building Footprint Extractor")
st.markdown(
    "Upload a 30 cm satellite or aerial image (GeoTIFF), and HWATA 2 will "
    "extract **regularized** building footprints as GeoJSON using the improved UNet model."
)


# ====================== HELPERS ======================
def _sha256(path: str) -> str:
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()


@st.cache_resource(show_spinner=False)
def fetch_model_file() -> str:
    """Ensure MODEL_PATH exists locally; download if missing."""
    if os.path.exists(MODEL_PATH):
        if MODEL_SHA256:
            if _sha256(MODEL_PATH) == MODEL_SHA256:
                return MODEL_PATH
            os.remove(MODEL_PATH)
        else:
            return MODEL_PATH

    st.info("Downloading improved HWATA2 model from GitHub Releases‚Ä¶")
    headers = {}
    if "GITHUB_TOKEN" in st.secrets:  # for private releases
        headers["Authorization"] = f"Bearer {st.secrets['GITHUB_TOKEN']}"

    with requests.get(MODEL_URL, headers=headers, stream=True, timeout=180) as r:
        r.raise_for_status()
        with open(MODEL_PATH, "wb") as f:
            for chunk in r.iter_content(1 << 20):
                if chunk:
                    f.write(chunk)

    if MODEL_SHA256:
        digest = _sha256(MODEL_PATH)
        assert digest == MODEL_SHA256, f"Model checksum mismatch: {digest}"

    return MODEL_PATH


def _infer_num_classes_from_state(state_dict: dict, default: int = 1) -> int:
    """Infer #classes from segmentation head weight tensor shape."""
    key = "segmentation_head.0.weight"
    if key not in state_dict and "module." + key in state_dict:
        key = "module." + key
    w = state_dict.get(key)
    if isinstance(w, torch.Tensor) and w.ndim == 4:
        return int(w.shape[0])
    return default


@st.cache_resource(show_spinner=False)
def load_model(model_path: str, device: torch.device):
    """Load UNet with inferred #classes from checkpoint."""
    raw = torch.load(model_path, map_location="cpu")
    state = raw["state_dict"] if isinstance(raw, dict) and "state_dict" in raw else raw
    num_classes = _infer_num_classes_from_state(state, default=1)

    model = smp.Unet(
        encoder_name="resnet34",
        encoder_weights="imagenet",
        in_channels=3,
        classes=num_classes,
        activation=None,
    ).to(device)

    model.load_state_dict(state, strict=True)
    model.eval()
    return model, num_classes


def logits_to_building_prob(logits: torch.Tensor) -> torch.Tensor:
    """Convert logits to building probability map."""
    if logits.shape[1] == 1:
        return torch.sigmoid(logits)
    return torch.softmax(logits, dim=1)[:, 1:2, ...]


# ====================== APP WORKFLOW ======================
uploaded_img = st.file_uploader("üìÇ Upload 30 cm Image (GeoTIFF)", type=["tif", "tiff"])

if uploaded_img:
    with NamedTemporaryFile(delete=False, suffix=".tif") as tmp_img:
        tmp_img.write(uploaded_img.read())
        tmp_img_path = tmp_img.name

    st.success("‚úÖ Image uploaded. Running prediction‚Ä¶")

    # Read raster
    with rasterio.open(tmp_img_path) as src:
        image = src.read([1, 2, 3])
        transform = src.transform
        height, width = src.height, src.width
        raster_crs = src.crs

    image = image.astype(np.float32) / 255.0

    # Fetch model
    try:
        model_path = fetch_model_file()
        st.caption(f"Model path: {model_path}")
    except Exception as e:
        st.error(f"Model fetch failed ‚ùå: {e}")
        os.remove(tmp_img_path)
        st.stop()

    # Load model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        model, num_classes = load_model(model_path, device)
        st.success(f"‚úÖ Model loaded (classes={num_classes})")
    except Exception as e:
        st.error(f"Model load failed ‚ùå: {e}")
        os.remove(tmp_img_path)
        st.stop()

    # Predict
    patch_size = 256
    full_pred = np.zeros((height, width), dtype=np.uint8)

    with torch.no_grad():
        for i in range(0, height - patch_size + 1, patch_size):
            for j in range(0, width - patch_size + 1, patch_size):
                patch = image[:, i:i+patch_size, j:j+patch_size]
                if patch.shape[1:] != (patch_size, patch_size):
                    continue
                t = torch.from_numpy(patch).float().unsqueeze(0).to(device)
                logits = model(t)
                prob = logits_to_building_prob(logits)
                mask = (prob.squeeze().cpu().numpy() > 0.5).astype(np.uint8)
                full_pred[i:i+patch_size, j:j+patch_size] = mask

    st.success("‚úÖ Prediction completed. Converting to GeoJSON‚Ä¶")

    # Convert mask ‚Üí polygons
    results = (
        {"properties": {"raster_val": v}, "geometry": s}
        for s, v in features.shapes(full_pred.astype(np.int16), transform=transform)
        if v == 1
    )
    gdf_pred = gpd.GeoDataFrame.from_features(list(results), crs=raster_crs)

    if len(gdf_pred) == 0:
        st.warning("No buildings detected. Try a clearer 30 cm image.")
        os.remove(tmp_img_path)
        st.stop()

    # Filter + regularize
    gdf_proj = gdf_pred.to_crs(gdf_pred.estimate_utm_crs())
    gdf_proj["area_m2"] = gdf_proj.geometry.area
    gdf_filtered = gdf_proj[gdf_proj["area_m2"] >= 10].to_crs(raster_crs)

    try:
        polygons = regularize_geodataframe(gdf_filtered)
    except Exception as e:
        st.warning(f"Regularization issue (showing raw polygons): {e}")
        polygons = gdf_filtered.copy()

    polygons = polygons.to_crs(epsg=4326)

    st.success("‚úÖ Building footprints extracted and regularized with HWATA 2.")

    # Map preview
    if st.checkbox("üåç Preview Building Footprints on Google Satellite"):
        bounds = polygons.total_bounds
        center_lat = (bounds[1] + bounds[3]) / 2
        center_lon = (bounds[0] + bounds[2]) / 2
        m = folium.Map(location=[center_lat, center_lon], zoom_start=18)
        folium.TileLayer(
            tiles="https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}",
            attr="Google Satellite",
        ).add_to(m)
        folium.GeoJson(
            polygons,
            style_function=lambda x: {"color": "magenta", "weight": 1.5, "fillOpacity": 0.3},
        ).add_to(m)
        st_folium(m, width=700, height=500)

    # Download
    st.subheader("üì• Download Extracted Footprints")
    st.download_button(
        "üìÑ Download GeoJSON",
        polygons.to_json(),
        file_name="hwata2_building_footprints.geojson",
        mime="application/geo+json",
    )

    os.remove(tmp_img_path)